{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework One:  Regular Expressions, Finite-State Automata, and Lexical Analysis\n",
    "\n",
    "Please provide answers to these problems by filling in this notebook and submitting it via websubmit by Tuesday 1/30 by midnight.  For the second problem, you must also submit an electronic version of your drawings, either using drawing software or by scanning or photographing hand drawings. Make sure in the latter case that the result is legible; if we can not understand your drawing, it will receive no credit. \n",
    "\n",
    "In addition to the above, submit your solution to Lab01 at the same time.\n",
    "\n",
    "You may submit by Wednesday midnight for a 20% penalty; after that no credit will be given. \n",
    "\n",
    "I will drop the lowest homework/lab combination at the end of the term.\n",
    "\n",
    "<b> Make sure to click Cell->Run All before submitting notebooks!!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem One\n",
    "\n",
    "Describe precisely and concisely the languages denoted by the following regular \n",
    "expressions (note: you will be graded on HOW well you understand the languages \n",
    "denoted, e.g., you will receive <i>no credit</i> if you describe the third language as \n",
    "\"the language of any number of <b>a</b>'s or <b>b</b>'s, followed by an <b>a</b>, \n",
    "followed by either an <b>a</b> or a <b>b</b>, followed by either an <b>a</b> or \n",
    "a <b>b</b>, followed by either an <b>a</b> or a <b>b</b>\"). \n",
    "<ol style=\"list-style-type: lower-alpha;\">\n",
    "      <li>$(1)^\\ast 1 (1)^\\ast 1 (1)^\\ast 1 (1)^\\ast 1 (1)^\\ast 1 (1)^\\ast$</li>\n",
    "      <li>$0 (0\\,|\\,1)^\\ast 1 (0\\,|\\,1)^\\ast 1 (0\\,|\\,1)^\\ast 0$ </li>\n",
    "      <li>$(a\\,|\\,b)^\\ast a(a\\,|\\,b)(a\\,|\\,b)(a\\,|\\,b)$   </li>\n",
    "      <li> $(b\\,|\\,ab)^\\ast(a\\,|\\,b^\\ast)$</li>\n",
    "      <li>$(aa\\,|\\,bb)^\\ast((ab\\,|\\,ba)(aa\\,|\\,bb)^\\ast(ab\\,|\\,ba)(aa\\,|\\,bb)^\\ast)^\\ast $</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "<ol style=\"list-style-type: lower-alpha;\">\n",
    "      <li>The language of all strings over {1} with at least 5 1's</li>\n",
    "      <li>The language of all strings over {0,1} which starts and ends with 0 with at least 2 1's</li>\n",
    "      <li>The language of all strings over {a,b} which ends with a length 4 string that starts with a and has only 9 combinations for the last 3 characters. </li>\n",
    "      <li>The language of all strings over {a,b} which does not have any consecitive a's in a row</li>\n",
    "      <li>The language of all strings over {a,b} with even number of a's and b's</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Two\n",
    "\n",
    "For each of the following, draw a DFA that accepts the language stated. Make sure to distinguish the start state (with a \">\") and any final states (as double circles). \n",
    "\n",
    "<ol style=\"list-style-type: lower-alpha;\">\n",
    "      <li>The language specified by the RE in Problem 1 (d).   </li>\n",
    "      <li>All strings of bits that contain <b>010</b> as a substring.</li>\n",
    "      <li>All binary strings which do NOT contain <b>010</b> as a substring</li>\n",
    "      <li>All strings over {a,b} with an even number of <b>a</b>'s and an odd number of <b>b</b>'s.</li>\n",
    "      <li>All strings of symbols from the set { /, $\\ast$, \", a } starting with the string <b>/$\\ast$</b> and ending in <b>$\\ast$/</b>, without an intervening <b>$\\ast$/</b> unless it appears inside a pair of quotes, and where all quotes must be matched (i.e., there have to be an even number of quotes). (Note: $\\ast$ is the ASCII asterisk, not the Kleene Star operator of regular expressions.) Thus:\n",
    "      \n",
    "<pre>   \n",
    "The following are legal:\n",
    "    /\\*aa\\*a\\*/                 \n",
    "    /\\*/\\*\\*/\n",
    "    /\\*\\*a\\*\"\\*/\"/\\*/\n",
    "    /\\*\"\"/\\*/             \n",
    "    /\\*a\"a\\*/a\"\\*/\n",
    "    \n",
    "and the following are illegal:\n",
    "\n",
    "    /\\*a\\*/\\*/\n",
    "    /\\*a/\\*\n",
    "    /\\*\"\\*/\n",
    "</pre>\n",
    "\n",
    "      </li>\n",
    "\n",
    "    </ol>\n",
    "\n",
    "Hint: There is a strong relationship between the DFA for (b) and the DFA for (c), can you see it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Three:  Lexical Analysis\n",
    "\n",
    "The job of the lexical analyzer is to take a string representing a program or expression to be evaluated, and turn it into a sequence or list of <i>tokens</i>, which are the smallest pieces of syntax, the \"words\" of the language. White space (blanks, tabs, new lines, etc.) is ignored. \n",
    "\n",
    "We want to distil the words down to their minimal representation: the type or category of token, and any other attributes (such as the value of an integer literal). The categories of tokens, represented as integers, are the following:\n",
    "<pre>\n",
    "id = 0\n",
    "integer = 1\n",
    "floating = 2\n",
    "plus = 3\n",
    "minus = 4\n",
    "mult = 5\n",
    "div = 6\n",
    "lparen = 7\n",
    "rparen = 8\n",
    "comma = 9\n",
    "</pre>\n",
    "We will represent tokens by tuples, where the first element gives the category, and, if necessary, the second element gives an essential attribute for that token. Some tokens have no attributes, and will be represented by a tuple with one element only.  \n",
    "\n",
    "We will use the following minimal set, which describes\n",
    "a simple subset of Python tokens, plus a special error token:\n",
    "<pre>\n",
    "(id,s)    an identifier (variable/function) spelled as the string s\n",
    "(integer,n) an integer numeral with value n (n is an actual int)\n",
    "(floating,k) a float numeral with value k (k is an actual float)\n",
    "(plus,)   arithmetic operator +\n",
    "(minus,)  arithmetic operator -\n",
    "(mult,)   arithmetic operator *\n",
    "(div,)    arithmetic operator /\n",
    "(lparen,) left parenthesis (\n",
    "(rparen,) right parenthesis )\n",
    "(comma,)  comma ,\n",
    "(error,)  error\n",
    "</pre>\n",
    "\n",
    "\n",
    "(Note that when there are no additional attributes, we use a tuple consisting of a single element.)\n",
    "\n",
    "For example, the arithmetic expression \"(x + (-4 * 3.4))\" would be converted into the following list of tokens:\n",
    "\n",
    "<pre>\n",
    "[ (lparen,), (id,'x'), (plus,), (lparen,), (minus,), (integer,4), \n",
    "  (mult,), (floating,3.4), (rparen,), (rparen,) ]\n",
    "</pre>\n",
    "  \n",
    "Note carefully that the second element of (floating,3.4) is an actual floating-point value, 3.4, and not the original substring \"3.4\" of the expression. During lexical analysis, the string representation of numeric literals needs to be converted into an actual number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Lexical Analyzer using a DFA\n",
    "\n",
    "### (A) Defining the language of tokens using regular expressions\n",
    "\n",
    "The first step (which we will do for you) is to specify what the syntax of each category of token is, which we will do with regular expressions:\n",
    "\n",
    "<blockquote>\n",
    "<b>id:</b> a non-empty string of digits, lower or upper-case letters, and underscores, but NOT starting with a digit:\n",
    "<pre>\n",
    "        [a..zA..Z\\_][0..9a..zA..Z_]&#42;\n",
    "</pre>\n",
    "\n",
    "<b>int:</b> either a single 0, or a non-empty string of digits NOT starting with a 0:\n",
    "<pre>\n",
    "        0 | [1..9][0..9]&#42;\n",
    "</pre>\n",
    "\n",
    "<b>float:</b> there are two possibilities: (i) same as an int, but with a decimal point and then 0 or more digits after the decimal point, or (ii) a decimal point followed by 1 or more digits:\n",
    "<pre>\n",
    "        ( 0 | [1..9][0..9]&#42; )'.'[0..9]&#42; | '.'[0..9]+     \n",
    "</pre>\n",
    "\n",
    "Note that we have put single quotes around the decimal point to emphasize that this is part of the alphabet, and not the same as the periods .. which just show the range of characters in a character class. <br><br>\n",
    "\n",
    "<b>plus:</b> a single '+':\n",
    "<pre>\n",
    "              +\n",
    "</pre>\n",
    "\n",
    "The other tokens are also just single characters, similar to plus. \n",
    "</blockquote>\n",
    "\n",
    "### (B) Creating recognizers for each of the categories of token \n",
    "\n",
    "To use DFAs to create a lexical analyzer for a programming language, we have to complete several steps:\n",
    "\n",
    "- Define DFAs equivalent to each of the regular expressions for the token categories, each with a single start state, one or more final states, and a single fail state; in fact, the DFAs for the single-character tokens are completely trivial. \n",
    "- Write a recognizer function for each category of token, based on the DFAs; in general, it is easier to use the technique of nested-if statements rather than transition tables, because of the large alphabet (e.g., identifiers involve 63 different characters, each requiring a column in the table). \n",
    "- if you wish, you may simplify the recognizers for the single-character tokens by simply testing if the string is equal to that character, e.g., \n",
    "<pre>\n",
    "    def legalPlusToken(s):\n",
    "        return s == '+'\n",
    "</pre>\n",
    "\n",
    "\n",
    "### (C) Creating a lexical analyzer based on the recognizers\n",
    "\n",
    "To use the token recognizers from the previous step in a real lexical analyzer, we have to solve one problem: You are not taking a string and determining if the whole string is a token, but scanning an entire program or expression from left to right and extracting the tokens you encounter as substrings along the way. \n",
    "\n",
    "In the interests of making this lab manageable, and to focus on the more interesting details, we will perform a simple but effective \"hack\" to facilitate this process:\n",
    "\n",
    "- Put white space (blanks) between each of the potential tokens in the input string by replacing each single-character token such as '+', '(', etc. with a three-character string consisting of a blank, the token, and another blank. Thus '+' would be replaced by ' + ' This can be easily done using the Python replace(...) function on strings. For example, you would convert \"(3+x)\" into \" (  3  +  x  ) \".\n",
    "\n",
    "- Separate out the potential tokens (which are the substrings consisting of sequential non-white-space characters) by splitting the string on white space, using the Python split() function. For example, you would convert \" (  3  +  x  ) \" into [\"(\", \"3\", \"+\", \"x\", \")\"], where all white space is removed and you have only <i>potential</i> tokens left. \n",
    "\n",
    "- Apply the recognizers to each of the potential tokens, and if a match is found, produce the appropriate token tuple. You will have to convert numeric tokens from strings to numeric form, and may use the Python functions int(...) and float(...) to do the conversions. For example, you would produce from our running example the list:\n",
    "\n",
    "        [ (lparen,), (integer,3), (plus,), (id,\"x\"), (rparen,) ]\n",
    "\n",
    "where at the appropriate point, you converted \"3\" into 3. For identifiers, no such conversion is necessary!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lexical Analyzer Code\n",
    "\n",
    "# Token Categories\n",
    "\n",
    "ident = 0\n",
    "integer = 1\n",
    "floating = 2\n",
    "plus = 3\n",
    "minus = 4\n",
    "mult = 5\n",
    "div = 6\n",
    "lparen = 7\n",
    "rparen = 8\n",
    "comma = 9\n",
    "error = 10\n",
    "\n",
    "# Code for pretty-printing tokens\n",
    "\n",
    "tokenCats = ['ident','integer','floating','plus','minus','mult','div',\n",
    "             'lparen','rparen','comma','error']\n",
    "\n",
    "def tokenToString(t):\n",
    "    if t[0] < 3:\n",
    "        return '(' + tokenCats[t[0]] + ',' + str(t[1]) + ')'\n",
    "    else:\n",
    "        return '(' + tokenCats[t[0]] + ',)'\n",
    "        \n",
    "def tokenListToString(lst):\n",
    "    res = '[ '\n",
    "    for t in lst[:-1]:\n",
    "        res = res + tokenToString(t) + ', '\n",
    "    res = res + tokenToString(lst[-1]) + ' ]'\n",
    "    return res\n",
    "\n",
    "# Code for lexer\n",
    "\n",
    "# Replace pass with your solution code!\n",
    "\n",
    "# put white space between each token and split into words\n",
    "def splitTokens(s):\n",
    "    for ch in ['(',')', ',', '+','-','/', '*']:\n",
    "        s = s.replace(ch, \" \"+ ch + \" \")\n",
    "    list1 = s.split();\n",
    "    return list1;\n",
    "def isIdToken(s):\n",
    "    if s == '0':\n",
    "        return True\n",
    "    state = 1\n",
    "    for ch in s:\n",
    "        if state == 1:\n",
    "            if 'a' <= ch <= 'z' or 'A' <= ch <= 'Z' or ch == '_':\n",
    "                state = 2\n",
    "            else:\n",
    "                return False\n",
    "        else:   # state = 2\n",
    "            if 'a' <= ch <= 'z' or 'A' <= ch <= 'Z' or '0' <= ch <= '9' or ch == '_':\n",
    "                state = 2\n",
    "            else:\n",
    "                return False            \n",
    "    return ( state == 2 )    \n",
    "            \n",
    "\n",
    "                        \n",
    "def isIntToken(s):\n",
    "    if s == '0':\n",
    "        return True\n",
    "    state = 1 \n",
    "    for ch in s:\n",
    "        if state == 1:\n",
    "            if '1' <= ch <= '9':\n",
    "                state = 2\n",
    "            else:\n",
    "                return False;\n",
    "        else:\n",
    "            if '0' <= ch <= '9':\n",
    "                state = 2;\n",
    "            else:\n",
    "                return False\n",
    "    return (state == 2)\n",
    "            \n",
    "    \n",
    "def isFloatToken(s):\n",
    "    state = 1 \n",
    "    for ch in s:\n",
    "        \n",
    "        if(state == 1):\n",
    "            if(isIntToken(ch)):\n",
    "                state = 2;\n",
    "            elif(ch == '.'):\n",
    "                state = 4;\n",
    "            else:\n",
    "                return False;\n",
    "        elif(state == 2):\n",
    "            if(ch == '.'):\n",
    "                state = 3;\n",
    "            else:\n",
    "                return False;\n",
    "        elif(state == 3):\n",
    "            if('0' <= ch <= '9'):\n",
    "                 state = 3;\n",
    "            else:\n",
    "                return False\n",
    "        elif(state == 4):\n",
    "            if('0' <= ch <= '9'):\n",
    "                state = 5;\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            if('0' <= ch <= '9'):\n",
    "                state = 5;\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "    return(state == 3 or state == 5)    \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "def isPlusToken(s):\n",
    "    return s == '+';\n",
    "\n",
    "def isMinusToken(s):\n",
    "    return s == '-';\n",
    "\n",
    "def isMultToken(s):\n",
    "    return s == '*';\n",
    "\n",
    "def isDivToken(s):\n",
    "    return s == '/';\n",
    "\n",
    "def isCommaToken(s):\n",
    "    return s == ',';\n",
    "\n",
    "def isLparanToken(s):\n",
    "    return s == '(';\n",
    "\n",
    "def isRparanToken(s):\n",
    "    return s == ')';\n",
    "\n",
    "\n",
    "def lexer(s):\n",
    "    string = [];\n",
    "    list2 = splitTokens(s);\n",
    "    for i in range(len(list2)):\n",
    "        if(isLparanToken(list2[i])):\n",
    "            string.append((7,));\n",
    "        elif(isRparanToken(list2[i])):\n",
    "            string.append((8,));\n",
    "        elif(isCommaToken(list2[i])):\n",
    "            string.append((9,));\n",
    "        elif(isDivToken(list2[i])):\n",
    "            string.append((6,));\n",
    "        elif(isMultToken(list2[i])):\n",
    "            string.append((5,));\n",
    "        elif(isMinusToken(list2[i])):\n",
    "            string.append((4,));\n",
    "        elif(isPlusToken(list2[i])):\n",
    "            string.append((3,));\n",
    "        elif(isFloatToken(list2[i])):\n",
    "            string.append((2, float(list2[i])));\n",
    "        elif(isIntToken(list2[i])):\n",
    "            string.append((1, int(list2[i])));\n",
    "        elif(isIdToken(list2[i])):\n",
    "            string.append((0, list2[i]));\n",
    "        else:\n",
    "            string.append((10,));\n",
    "    return string;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ (ident,x), (ident,HiThErE), (ident,y3Y4), (ident,_fl_ag4_), (error,), (error,) ]\n",
      "\n",
      "[ (integer,4), (integer,2304500), (integer,0), (error,), (error,) ]\n",
      "\n",
      "[ (floating,3.14), (floating,3.0), (floating,3.0), (floating,0.0), (floating,0.034), (error,), (error,) ]\n",
      "\n",
      "[ (plus,), (minus,), (mult,), (div,), (lparen,), (rparen,), (comma,) ]\n",
      "\n",
      "[ (lparen,), (integer,4), (plus,), (lparen,), (ident,flag_4), (div,), (minus,), (integer,4), (rparen,), (mult,), (floating,3.4), (minus,), (integer,0), (rparen,) ]\n",
      "\n",
      "Note: in the first three, the last two tokens are errors and\n",
      "the previous are correct; for the last two all are correct.\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "\n",
    "print(tokenListToString(lexer('x HiThErE y3Y4 _fl_ag4_ 4flag flag.4')))\n",
    "print()\n",
    "print(tokenListToString(lexer('4 2304500 0 03434 00 ')))\n",
    "print()\n",
    "print(tokenListToString(lexer('3.14  3.  3.0  0.  .0340  03.4  . ')))\n",
    "print()\n",
    "print(tokenListToString(lexer('+-*/(),')))\n",
    "print()\n",
    "print(tokenListToString(lexer('(4+(flag_4 / -4) * 3.4 - 0 )')))\n",
    "print(\"\\nNote: in the first three, the last two tokens are errors and\") \n",
    "print(\"the previous are correct; for the last two all are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
